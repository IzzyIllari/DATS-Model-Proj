length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data$Age))
loadPkg(dplyr)
olympic_data_1 <- olympic_data %>% select(c(-ID, -Name, -Last.Name, -Decade, -Height, -Weight, -Team, -Games, -City, -Event, -GDP, -Population, -Medal, -First.Name))
head(olympic_data_1)
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
loadPkg(psych)
pairs(olympic_data_1)
loadPkg(ggplot2)
ggplot(olympic_data_1, aes(x=Age, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=Age, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=BMI, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=BMI, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=GDPpC, fill=Medal.No.Yes)) + geom_histogram(bins = 30)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=GDPpC, fill=Medal.No.Yes)) + geom_boxplot()
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data$Age))
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
loadPkg(ggplot2)
ggplot(olympic_data_1, aes(x=Age, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=Age, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=BMI, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=BMI, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=GDPpC, fill=Medal.No.Yes)) + geom_histogram(bins = 30)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=GDPpC, fill=Medal.No.Yes)) + geom_boxplot()
set.seed(1)
train_rows = sample(1:nrow(olympic_data_1), round(0.7 * nrow(olympic_data_1), 0),  replace = FALSE)
length(train_rows) / nrow(olympic_data_1)
data_train = olympic_data_1[train_rows, ]
data_test = olympic_data_1[-train_rows, ]
nrow(data_train)
nrow(data_test)
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
OlympicsLogit_1 <- glm(survived ~ age + pclass, data = titanic_orig, binomial(link = "logit") )
colnames(olympic_data_1)
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
OlympicsLogit_1 <- glm(Medal.No.Yes ~ NOC + Year + Sex + Age + BMI + GDPpC + Season + Sport, data = data_train, binomial(link = "logit") )
summary(OlympicsLogit_1)
loadPkg(dplyr)
olympic_data_1 <- olympic_data %>% select(c(-ID, -Name, -Last.Name, -Decade, -Height, -Weight, -Team, -Games, -City, -Event, -GDP, -Population, -Medal, -First.Name, -NOC))
head(olympic_data_1)
loadPkg(dplyr)
olympic_data_1 <- olympic_data %>% select(c(-ID, -Name, -Last.Name, -Height, -Weight, -Team, -Games, -City, -Event, -GDP, -Population, -Medal, -First.Name))
head(olympic_data_1)
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Decade))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Decade))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$BMI))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Decade))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$BMI))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
loadPkg(ggplot2)
ggplot(olympic_data_1, aes(x=Age, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=Age, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=BMI, fill=Medal.No.Yes)) + geom_histogram(bins = 16)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=BMI, fill=Medal.No.Yes)) + geom_boxplot()
ggplot(olympic_data_1, aes(x=GDPpC, fill=Medal.No.Yes)) + geom_histogram(bins = 30)
ggplot(olympic_data_1, aes(x=Medal.No.Yes, y=GDPpC, fill=Medal.No.Yes)) + geom_boxplot()
set.seed(1)
train_rows = sample(1:nrow(olympic_data_1), round(0.7 * nrow(olympic_data_1), 0),  replace = FALSE)
length(train_rows) / nrow(olympic_data_1)
data_train = olympic_data_1[train_rows, ]
data_test = olympic_data_1[-train_rows, ]
nrow(data_train)
nrow(data_test)
colnames(olympic_data_1)
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
OlympicsLogit_1 <- glm(Medal.No.Yes ~ Decade + Sex + Age + BMI + GDPpC + Season + Sport, data = data_train, binomial(link = "logit") )
summary(OlympicsLogit_1)
View(summary(OlympicsLogit_1))
View(OlympicsLogit_1)
expcoeff <- exp(coef(OlympicsLogit_1))
expcoeff
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
confusion_matrix = confusion_matrix(OlympicsLogit_1)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2])/confusion_matrix[3,3]
precision <- confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[1,2])
recall <- confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[2,1])
fscore <- (2*precision*recall)/sum(precision, recall)
colnames(data_test)
y_predict = predict.glm(OlympicsLogit_1, data_test[3:6, 8:10])
y_predict = predict.glm(OlympicsLogit_1, data_test[,c(3:6, 8:10)])
y_predict_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_YN <- ifelse(y_predict_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm1 = confusionMatrix(as.factor(y_predict_YN), reference = data_test[, "Medal.No.Yes"] )
y_predict = predict.glm(OlympicsLogit_1, data_test[,c(3:6, 8:10)])
y_predict_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_YN <- ifelse(y_predict_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm1 = confusion_matrix(as.factor(y_predict_YN), reference = data_test[, "Medal.No.Yes"] )
loadPkg(caret)
y_predict = predict.glm(OlympicsLogit_1, data_test[,c(3:6, 8:10)])
y_predict_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_YN <- ifelse(y_predict_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm1 = confusionMatrix(as.factor(y_predict_YN), reference = data_test[, "Medal.No.Yes"] )
cm1
cm1$byClass
loadPkg(caret)
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
y_predict_train = predict.glm(OlympicsLogit_1, data_train[,c(3:6, 8:10)])
y_predict_train_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_train_YN <- ifelse(y_predict_train_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm_train = confusionMatrix(as.factor(y_predict_train_YN), reference = data_train[, "Medal.No.Yes"] )
loadPkg(caret)
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
y_predict_train = predict.glm(OlympicsLogit_1, data_train[,c(3:6, 8:10)])
y_predict_train_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_train_YN <- ifelse(y_predict_train_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm_train = confusionMatrix(y_predict_train_YN, reference = data_train[, "Medal.No.Yes"] )
loadPkg(caret)
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
y_predict_train = predict.glm(OlympicsLogit_1, data_train[,c(3:6, 8:10)])
y_predict_train_trans <- 1/(1+exp(-y_predict))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_train_YN <- ifelse(y_predict_train_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm_train = confusionMatrix(as.factor(y_predict_train_YN), reference = data_train[, "Medal.No.Yes"] )
loadPkg(caret)
y_predict = predict.glm(data_train, data_test[,c(3:6, 8:10)])
loadPkg(caret)
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
y_predict_train = predict.glm(OlympicsLogit_1, data_train[,c(3:6, 8:10)])
y_predict_train_trans <- 1/(1+exp(-y_predict_train))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_train_YN <- ifelse(y_predict_train_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm_train = confusionMatrix(as.factor(y_predict_train_YN), reference = data_train[, "Medal.No.Yes"] )
cm_train
cm_train$byClass
loadPkg(caret)
y_predict_test = predict.glm(OlympicsLogit_1, data_test[,c(3:6, 8:10)])
y_predict_test_trans <- 1/(1+exp(-y_predict_test))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_test_YN <- ifelse(y_predict_test_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm1 = confusionMatrix(as.factor(y_predict_test_YN), reference = data_test[, "Medal.No.Yes"] )
cm1
cm1$byClass
loadPkg(pROC)
data_test$prob=y_predict_test_trans
h <- roc(type~prob, data=data_test)
loadPkg(pROC)
data_test$prob=y_predict_test_trans
h <- roc(Medal.No.Yes~prob, data=data_test)
auc(h)
plot(h)
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
loadPkg = function(pkg, character.only = FALSE) {
if (!character.only) { pkg <- as.character(substitute(pkg)) }
pkg <- ifelse(!character.only, as.character(substitute(pkg)) , pkg)
if (!require(pkg,character.only=T, quietly =T)) {  install.packages(substitute(pkg),dep=T); if(!require(pkg,character.only=T)) stop("Package not found") }
}
loadPkg(knitr)
# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) {
if(!character.only) { pkg <- as.character(substitute(pkg)) }
search_item <- paste("package", pkg,sep = ":")
while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) }
}
loadPkg(knitr)
loadPkg(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
loadPkg(xtable)
loadPkg(kableExtra)
loadPkg(stringi)
xkabledply = function(smmry, title='Caption', pos='left') { # Thanks Ryan Longmuir for the codes
smmry %>%
xtable() %>%
kable(caption = title, digits = 4) %>%
kable_styling(position = "center") %>%
kable_styling(bootstrap_options = "striped", full_width = F,
position = pos)
}
xkablesummary = function(df) {
#' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes.
#` If the categorical variables has less than 6 levels, the function will still run without error.
#' ELo 202003 GWU DATS
#' version 1
#' @param df The dataframe.
#' @return The summary table for display, or for knitr to process into other formats
#' @examples
#' xkablesummary( faraway::ozone )
#' xkablesummary( ISLR::Hitters )
s = summary(df) %>%
apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
colnames(s) <- stringr::str_trim(colnames(s))
if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max')
} else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
s %>%
xkabledply("Table: Statistics summary.", "center")
}
xkablevif = function(model) {
#' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model.
#' ELo 202003 GWU DATS
#' version 1
#' @param df The dataframe.
#' @return The summary table for display, or for knitr to process into other formats
#' @examples
#' xkablevif( model )
vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) # remove intercept to set column names
vifs[] = faraway::vif(model) # set the values
vifs %>%
xtable() %>%
kable(caption = "VIFs of the model", digits = 4, col.names = 'VIF') %>% # otherwise it will only has the generic name as 'V1' for the first vector in the table
kable_styling(position = "center") %>%
kable_styling(bootstrap_options = "striped", full_width = F,
position = "left")
}
olympic_data <- data.frame(read.csv("olympic_data.csv"))
olympic_data$BMI.Category <- as.factor(olympic_data$BMI.Category)
olympic_data$Medal.No.Yes <- as.factor(olympic_data$Medal.No.Yes)
head(olympic_data)
str(olympic_data)
# summary1 = xkabledply(olympic_data)
# summary1
loadPkg(dplyr)
olympic_data_1 <- olympic_data %>% select(c(-ID, -Name, -Last.Name, -Height, -Weight, -Team, -Games, -City, -Event, -GDP, -Population, -Medal, -First.Name))
head(olympic_data_1)
sum(is.na(olympic_data_1$NOC))
sum(is.na(olympic_data_1$Decade))
sum(is.na(olympic_data_1$Year))
sum(is.na(olympic_data_1$Age))
sum(is.na(olympic_data_1$GDPpC))
sum(is.na(olympic_data_1$BMI.Category))
sum(is.na(olympic_data_1$BMI))
sum(is.na(olympic_data_1$Season))
sum(is.na(olympic_data_1$Sport))
sum(is.na(olympic_data_1$Medal.No.Yes))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$Age))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$GDPpC))
olympic_data_1 <- olympic_data_1 %>% filter(!is.na(olympic_data_1$BMI))
length(olympic_data_1$GDPpC)
sum(is.na(olympic_data_1$GDPpC))
loadPkg(psych)
pairs(olympic_data_1)
set.seed(1)
train_rows = sample(1:nrow(olympic_data_1), round(0.7 * nrow(olympic_data_1), 0),  replace = FALSE)
length(train_rows) / nrow(olympic_data_1)
data_train = olympic_data_1[train_rows, ]
data_test = olympic_data_1[-train_rows, ]
nrow(data_train)
nrow(data_test)
loadPkg(regclass)
loadPkg(ResourceSelection)
OlympicsLogit_1 <- glm(Medal.No.Yes ~ Decade + Sex + Age + BMI + GDPpC + Season + Sport, data = data_train, binomial(link = "logit") )
summary(OlympicsLogit_1)
expcoeff <- exp(coef(OlympicsLogit_1))
expcoeff
LogitHoslem <- hoslem.test(data_train$Medal.No.Yes, fitted(OlympicsLogit_1))
LogitHoslem <- hoslem.test(data_train$Medal.No.Yes, fitted(OlympicsLogit_1))
LogitHoslem
loadPkg(pscl) # use pR2( ) function to calculate McFadden statistics for model eval
OlympicsLogitpr2 = pR2(OlympicsLogit_1)
OlympicsLogitpr2
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = 'markup', message = F)
knitr::opts_chunk$set(warning = F, results = 'hide', message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
loadPkg = function(pkg, character.only = FALSE) {
if (!character.only) { pkg <- as.character(substitute(pkg)) }
pkg <- ifelse(!character.only, as.character(substitute(pkg)) , pkg)
if (!require(pkg,character.only=T, quietly =T)) {  install.packages(substitute(pkg),dep=T); if(!require(pkg,character.only=T)) stop("Package not found") }
}
loadPkg(knitr)
# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) {
if(!character.only) { pkg <- as.character(substitute(pkg)) }
search_item <- paste("package", pkg,sep = ":")
while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) }
}
titanic_orig <- data.frame(read.csv("titanic.csv"))
head(titanic_orig)
summary(titanic_orig)
loadPkg(dplyr)
sum(is.na(titanic_orig$age))
sum(is.na(titanic_orig$survived))
length(titanic_orig$age)
titanic_orig <- titanic_orig %>% filter(!is.na(titanic_orig$age))
sum(is.na(titanic_orig$age))
head(titanic_orig, 10)
titanic_orig$parch <- factor(titanic_orig$parch)
titanic_orig$survived <- factor(titanic_orig$survived)
titanic_orig$sibsp <- factor(titanic_orig$sibsp)
titanic_orig$pclass <- factor(titanic_orig$pclass)
head(titanic_orig, 10)
loadPkg("ggplot2")
ggplot(titanic_orig, aes(x=age, fill=survived)) + geom_histogram(bins = 16)
ggplot(titanic_orig, aes(x=survived, y=age, fill=survived)) + geom_boxplot()
bins <-  seq(0, 80, by=10)
names <- c("0-10", "10-20", "20-30", "30-40", "40-50", "50-60", "60-70", "70-80")
titanic_orig$age_cats <- cut(titanic_orig$age, breaks = bins, labels = names)
titanic_orig$age_cats <- factor(titanic_orig$age_cats)
ctable1 = xtabs(~ survived + age_cats, data = titanic_orig)
chisqres1 = chisq.test(ctable1)
chisqres1
loadPkg(expss)
ggplot(titanic_orig, aes(x=sex, fill=survived)) + stat_count()
t0 <- count(titanic_orig, sex)
males <- titanic_orig %>% filter(sex=='male')
females <- titanic_orig %>% filter(sex=='female')
t1<-count(males, survived)
t2<-count(females, survived)
m0<-round((t1[1,2]/t0[2,2])*100, 0)
m1<-round((t1[2,2]/t0[2,2])*100, 0)
f0<-round((t2[1,2]/t0[1,2])*100, 0)
f1<-round((t2[2,2]/t0[1,2])*100, 0)
ctable2 = xtabs(~ survived + sex, data = titanic_orig)
chisqres2 = chisq.test(ctable2)
chisqres2
ggplot(titanic_orig, aes(x=pclass, fill=survived)) + stat_count()
t0 <- count(titanic_orig, pclass)
c1 <- titanic_orig %>% filter(pclass=='1')
c2 <- titanic_orig %>% filter(pclass=='2')
c3 <- titanic_orig %>% filter(pclass=='3')
t1<-count(c1, survived)
t2<-count(c2, survived)
t3<-count(c3, survived)
c10<-round((t1[1,2]/t0[1,2])*100, 0)
c11<-round((t1[2,2]/t0[1,2])*100, 0)
c20<-round((t2[1,2]/t0[2,2])*100, 0)
c21<-round((t2[2,2]/t0[2,2])*100, 0)
c30<-round((t3[1,2]/t0[3,2])*100, 0)
c31<-round((t3[2,2]/t0[3,2])*100, 0)
ctable3 = xtabs(~ survived + pclass, data = titanic_orig)
chisqres3 = chisq.test(ctable3)
chisqres3
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
surviveLogit <- glm(survived ~ age + pclass, data = titanic_orig, binomial(link = "logit") )
surviveNullLogit <- glm(survived ~ 1, data = titanic_orig, family = "binomial")
summary(surviveLogit)
expcoeff <- exp(coef(surviveLogit))
expcoeff
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
confusion_matrix = confusion_matrix(surviveLogit)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2])/confusion_matrix[3,3]
precision <- confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[1,2])
recall <- confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[2,1])
fscore <- (2*precision*recall)/sum(precision, recall)
LogitHoslem <- hoslem.test(titanic_orig$survived, fitted(surviveLogit))
prob <- predict(surviveLogit, type = c("response"))
titanic_orig$prob=prob
h <- roc(survived~prob, data=titanic_orig)
auc(h)
plot(h)
loadPkg(pscl) # use pR2( ) function to calculate McFadden statistics for model eval
surviveLogitpr2 = pR2(surviveLogit)
surviveLogitpr2
loadPkg(pscl) # use pR2( ) function to calculate McFadden statistics for model eval
OlympicsLogitpr2 = pscl::pR2(OlympicsLogit_1)
OlympicsLogitpr2
loadPkg(caret)
# accuracy = TP+TN/All
# precision = TP / (TP + FP)
# recall = TP / (TP + FN)
# Fscore = (2*Precision*Recall) / sum(Precision, Recall)
y_predict_train = predict.glm(OlympicsLogit_1, data_train[,c(3:6, 8:10)])
y_predict_train_trans <- 1/(1+exp(-y_predict_train))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_train_YN <- ifelse(y_predict_train_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm_train = confusionMatrix(as.factor(y_predict_train_YN), reference = data_train[, "Medal.No.Yes"] )
cm_train
cm_train$byClass
loadPkg(caret)
y_predict_test = predict.glm(OlympicsLogit_1, data_test[,c(3:6, 8:10)])
y_predict_test_trans <- 1/(1+exp(-y_predict_test))
# I'll turn the probabilities into classifications by thresholding at 0.5.
y_predict_test_YN <- ifelse(y_predict_test_trans > 0.5, "1", "0")
# Now I can compare with test results in a confusion matrix
cm1 = confusionMatrix(as.factor(y_predict_test_YN), reference = data_test[, "Medal.No.Yes"] )
cm1
cm1$byClass
loadPkg(pROC)
data_test$prob=y_predict_test_trans
h <- roc(Medal.No.Yes~prob, data=data_test)
auc(h)
plot(h)
